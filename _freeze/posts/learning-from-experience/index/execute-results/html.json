{
  "hash": "9246fdba8a603cef21bcf5319a44a359",
  "result": {
    "markdown": "---\ntitle: \"You don't understand [stats]. You get used to them\"\nauthor: \"Joseph Powers\"\ndate: \"2023-12-17\"\ncategories: [simulation, learning, data simulation, modeling, binomial]\nimage: \"roman_roads.png\"\ndraft: true\nwarning: false\nmessage: false\n---\n\n\nI recently learned the Von Neumann quote, \"in mathematics, you don't understand things. You just get used to them.\" This perfectly captures my own experience of 8 unproductive years desperately trying to understand statistics followed by 3 very productive years getting used to them by leveraging data simulation. If you feel like you've been in a long (or short) rut trying to develop better intuition about statistics, then I hope that this post will bring some welcome relief. \n\nData simulation is key to \"getting used to stats\" for two reasons: \n\n1. You don't need to hunt around for the data you need to practice new methods. Want to learn to model interactions, prognostic covariates or hierarchical models ... make it up. \n\n2. You actually know the underlying truth (i.e., parameters) in simulated data. In real data you usually do not. \n\n3. Because you cam simulate unlimited datasets from the same parameters you truly (and quickly) can get used to how sample statistics behave.\n\nAll these points matter but the third was most powerful of all for me. After a few weeks of \"simulated data as a way of life\" [source][(Gelman et al, 2021) I noticed that I was forming stronger expectations about how sample statistics should behave. These expectations weren't memorized it felt more like training my ear in music or training my pallette as a chef. I was finally getting enough exposure with feedback that my intuitions were taking shape. What feedback? The feedback loop is that I knew the underlying truth in my data so I could rapidly and continuously see how I drifted away from or confirmed that truth as I played with simulations. \n\nEnough talk. Let's do. \n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(scales, tidyverse, glue)\nbreadcrumbs::source_directory(here::here('R'))\n```\n:::\n\n\n\n# random number likelihood functions are your chef's knives \n\n\n::: {.cell}\n\n```{.r .cell-code}\nSAMPLES <- 5000\nTRIALS_1k <- 1000\nPROB_EVENT <- .5\n```\n:::\n\n\nSo we have a known truth, that our probability of some event is 50%. Could be conversions, basketball freethrows, coin tosses, who cares. Let's say my tech product has a 50% conversion rate. \n\nLet's ask a good \"getting used to stats\" question.\n\nIf I have a true underlying conversion rate of 0.5, what is the range of possible outcomes I might see in a sample of 1000 customers?\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\n\nsim_events <- rbinom(n=SAMPLES, size=TRIALS_1k, prob=PROB_EVENT)\nsim_rates <- sim_events / TRIALS_1k\n```\n:::\n\n\nThis is often more intutive to see as rates which we can can by dividing each simulated events count by the number of trials: \n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim_rates) %>%\n    {ggplot(data = ., aes(x = sim_rates)) + \n    geom_histogram(bins=250) + \n    scale_x_continuous(\n        breaks = seq(0,1,.02),\n        labels = ~percent(.x, 1L), \n        limits = c(.4,.6)\n    ) +\n    labs(\n        title = str_wrap(glue(\"All simulated samples of {TRIALS_1k} customers had rates between {format_range(.$sim_rates, .percent=TRUE, .accuracy = 0.1)}\")),\n        y = \"Count\",\n        x = glue(\"Observed Outcomes in Samples of {TRIALS_1k} Customers\"))}\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nWhat might we expect from a larger sample? \n\n::: {.cell}\n\n```{.r .cell-code}\nTRIALS_10k <- 10e3\n\nsim_events <- rbinom(n=SAMPLES, size=TRIALS_10k, prob=PROB_EVENT)\nsim_rates <- sim_events / TRIALS_10k\n\ntibble(sim_rates = sim_events / TRIALS_10k) %>%\n    {ggplot(data = ., aes(x = sim_rates)) + \n    geom_histogram(bins=200) + \n    scale_x_continuous(\n        breaks = seq(0,1,.01),\n        labels = ~percent(.x, 1L), \n        limits = c(.4,.6)\n    )  +\n    labs(\n        title = str_wrap(glue(\"All simulated samples of {comma(TRIALS_10k)} customers had rates between {format_range(.$sim_rates, .percent=TRUE, .accuracy = 0.1)}\")),\n        subtitle = \"Larger samples have a narrower range of possible outcomes\",\n        y = \"Count\",\n        x = glue(\"Potential Outcomes from Samples of {TRIALS_10k} Customers\")) \n        }\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nWhat is the aha moment? Well, imagine you are at a company that has a 50% conversion rate, and in one week's report the conversions dip to 1-2]%. Is this cause for alarm? Absolutely not, this is expected behavior in such a distribution\n\n\nIf you want to get used to statistics, I highly recommend you invest your time in data simulation. Simulating data from  base R functions `rbinom()`, `rnorm()`, etc., is a very efficient way to develop a deep and practical intuition about the statistical models they power. \n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(glue, scales, tidyverse)\n```\n:::\n\n\n\n# WORKING HERE\n# The Binomial Distribution\nBinomial distributions are rampant, and their initial simplicity can belie complexities that I will probe in this post.\n\n$$Binomial(k, p)$$\n$$k = trials\\ per\\ sample$$\n$$p = Pr(success\\ event\\ on\\ each\\ trial)$$\n\n$$Binomial(k=1e4,\\ p=.75)$$\n\nIt's worth emphasizing some vocabulary: \n\n* a **parameter** summarizes an underlying truth (usually unknowable except when performing simulations). E.g., My customers have a 75% probability of converting. \n\n* a **statistic** summarizes a sample of observable data. E.g., 74.2% of my customers converted last month. \n\n* in a sense, **parameters** generate samples that can be summarized by **statistics**, and the generation process has some interesting propperties.  \n\nI'll specify two parameters for a Binomial distribution: \n\n::: {.cell}\n\n```{.r .cell-code}\nK <- 1e3\nP <- 0.75\n```\n:::\n\n\nGiven these parameters, what range of stats might I expect in my samples from the distribution $Binomial(k=1e4,\\ p=0.75)$? \n\n### rbinom()\nA really simple way to address this question is with `rbinom()`.\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\n\nrbinom(n=1, size=K, prob=P)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 760\n```\n:::\n:::\n\n\nIf I run the same code again I will get a different sample.\n\n::: {.cell}\n\n```{.r .cell-code}\nrbinom(n=1, size=K, prob=P)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 738\n```\n:::\n:::\n\n\nI can also run this as a series of Bernoulli trials\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(45)\n\nmy_bernoulli <- rbinom(n=K, size=1, prob=P)\n\nmy_bernoulli[1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 1 1 1 1 1 1 1 1 1 1\n```\n:::\n:::\n\n\nThe Bernoulli is a special case of the Binomial ($Bernoulli(p=0.75) = Binomial(k=1,\\ p=0.75)$), and the sum of the $k$ Bernoulli trials would represent the outcome for one Binomial sample with $k$ trials. \n\n::: {.cell}\n\n```{.r .cell-code}\nsum(my_bernoulli)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 771\n```\n:::\n:::\n\n\nThere are enormous efficiencies to be gained from using Binomial distribution rather than the Bernoulli when simulating thousands or millions of samples. But know that the results would converge. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nN <- 4e3 # samples to simulate\n```\n:::\n\n\nSo now I will simulate 4000 samples from $Binomial(k=1e4, p=0.75)$\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_events <- rbinom(n=N, size=K, prob=P)\n```\n:::\n\n\nrbinom() just outputs a long vector of event tallies. Let's look at 10. \n\n::: {.cell}\n\n```{.r .cell-code}\nsim_events[1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 740 717 736 765 774 756 769 705 727 735\n```\n:::\n:::\n\n\nThe full vector of sample stats is easier to visualize in a plot\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim_events) |> \n    ggplot(aes(x = sim_events)) + \n    geom_histogram(bins=200)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nSo we can see that the distribution $Binomial(1e4, 0.75)$ generated samples with 699 to 813 events. We could describe this range of potential samples further:\n\n::: {.cell}\n\n```{.r .cell-code}\npaste0('mean = ', mean(sim_events) |> round(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"mean = 750.124\"\n```\n:::\n\n```{.r .cell-code}\npaste0('SEM = ', sd(sim_events) |> round(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"SEM = 13.578\"\n```\n:::\n\n```{.r .cell-code}\nquantile(sim_events, c(0.025, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 2.5% 97.5% \n  724   776 \n```\n:::\n:::\n\n\nSo `rbinom()` will output a tally of events, but often we are more interested in the rate this represents.  \n\n::: {.cell}\n\n```{.r .cell-code}\nsim_rates <- sim_events / K\n\npaste0('mean = ', mean(sim_rates)  |> round(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"mean = 0.75\"\n```\n:::\n\n```{.r .cell-code}\npaste0('SEM = ', sd(sim_rates) |> round(5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"SEM = 0.01358\"\n```\n:::\n\n```{.r .cell-code}\nquantile(sim_rates, c(0.025, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 2.5% 97.5% \n0.724 0.776 \n```\n:::\n:::\n\n\nYou should note above that I have computed the Standard Error of the Mean using `sd(sim_rates)` because `sim_rates` actually represents a *sampling distribution*. `sim_rates` is a sample of samples ... and so the Central Limit Theorum kicks in to ensure \n\n* it approximates normality regardless that the raw data is not normal (the raw data are 0s and 1s), \n\n* its mean approximates the true parameter mean of 0.75\n\n* the standard deviation of the sampling distribution is the standard error of the mean\n\nNormally you would have tried to mathematically estimate such results from just one real sample: \n\n::: {.cell}\n\n```{.r .cell-code}\n# grab just one random sample\nrate1 <- sim_rates[[18]]\n\n# std dev of binomial data is sqrt(p*(1-p))\nsd1 <- sqrt(rate1 * (1-rate1))\n\n# convert std dev to SEM\nse1 <- sd1 / sqrt(K)\n\nse1 |> round(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.01401\n```\n:::\n:::\n\n\nThat's pretty cool how close we can estimate the standard error with just one sample. \n\n## Summary: \nSo we've just simulated some data using `rbinom()` and explored its properties. \n\n# dbinom()\n`rbinom()` can generate simulated data and I find its results to be very intuitive. Density (`dbinom()`) eluded my intuition for an embarrassingly long time. Until I saw how the results converged with `rbinom()`.\n\nUsing the same distribution $Binomial(1e4, 0.75)$, let's first consider the range of stats that we could observe. In 1,000 trials I can only observe between 0 & 1,000 events, so this defines my range of observable stats. \n\n::: {.cell}\n\n```{.r .cell-code}\nrange_of_event_tallies <- 0:K\n```\n:::\n\n\nOf course, some of those stats are more likely to manifest in a sample than others. If I have a true underlying success rate of 75% this could more readily manifest as 748 events than it could manifest as 992 events in a sample of 1000 trials. 748 events certainly feels more likely, even though 992 events is not impossible. \n\nWe can use the `dbinom()` function to return the likelihood of each possible event tally from $Binomial(1e4, 0.75)$.\n\n::: {.cell}\n\n```{.r .cell-code}\nlikelihood_of_event_tallies <- \n    dbinom(\n        range_of_event_tallies,\n        size = K,\n        prob = P\n    )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_density <- \n    tibble(\n        range_of_event_tallies,\n        likelihood_of_event_tallies\n    )\n\ndf_density |> \n    ggplot(aes(x = range_of_event_tallies, y = likelihood_of_event_tallies)) + \n    geom_line() + \n    labs(\n        title = str_wrap(glue('Samples from Binomial({K}, {P}) cover a pretty narrow range centered around {K*P} events'), 65)\n    )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\nAs we said earlier, it's usually easier to think about these stats in terms of rates rather than tallies, so we'll just divide the event_tallies by the trial count (`K`).\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_density <- \n    df_density |> \n    mutate(range_of_sample_rates = range_of_event_tallies / K)\n\np_rate <- \n    df_density |> \n    ggplot(aes(x = range_of_sample_rates, y = likelihood_of_event_tallies)) + \n    geom_line() + \n    scale_x_continuous(labels = percent) + \n    labs(\n        title = str_wrap(glue('Samples from Binomial({K}, {P}) cover a pretty narrow range centered around {percent(P)}'), 65)\n    )\n\np_rate\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\np_rate + \n    coord_cartesian(xlim = c(.7, .8)) + \n    labs(title = str_wrap(\"Zooming in I can eyeball that about 95% of potential samples will fall between 72.5% & 77.5%\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\nBut don't take my word for it. Let's see where the 2.5% and 97.5%-iles of this distribution fall. \n\n::: {.cell}\n\n```{.r .cell-code}\nci95_events <- qbinom(c(0.025, 0.975), K, P)\n\nci95_rates <- ci95_events / K\n\nci95_rates\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.723 0.777\n```\n:::\n:::\n\n\nNot bad.\n\nNow for the kicker: We can arrive at nearly these same values through simulation or density functions. Let's revisit our simulated sampling distribution `sim_rates`:\n\n::: {.cell}\n\n```{.r .cell-code}\nquantile(sim_rates, c(0.025, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 2.5% 97.5% \n0.724 0.776 \n```\n:::\n:::\n\n\nTell me you don't have goose bumps!\n\n# All roads lead to Rome:\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_sim <- \n    tibble(range_of_sample_rates = sim_rates) |> \n    count(range_of_sample_rates) |> \n    mutate(prop = n / sum(n))\n\n# this join works very well to create a double y-axis\nleft_join(df_density, df_sim, by = 'range_of_sample_rates') |> \n    ggplot(aes(x = range_of_sample_rates, y = likelihood_of_event_tallies)) + \n    geom_col(aes(x=range_of_sample_rates, y=prop)) + \n    geom_line(color = 'red', linewidth=1) + \n    scale_x_continuous(\n        breaks = pretty_breaks(10),\n        labels = ~percent(.x,1)\n        ) + \n    labs(\n        x = \"Potential Sample Rates\",\n        y = \"Likelihood of Sample Rate\",\n        title = str_wrap(\"Note the convergence of rbinom()'s histogram and dbinom()'s line\"), 65) + \n    coord_cartesian(xlim = c(.7,.8))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}