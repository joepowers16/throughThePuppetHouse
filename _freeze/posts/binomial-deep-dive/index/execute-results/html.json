{
  "hash": "0eac0c6666ddd354fda7c93184012782",
  "result": {
    "markdown": "---\ntitle: \"Binomial Deep Dive\"\nauthor: \"Joseph Powers\"\ndate: \"2023-12-16\"\ncategories: [simulation, learning, data simulation, modeling, binomial]\nimage: \"roman_roads.png\"\ndraft: false\nwarning: false\nmessage: false\n---\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(glue, scales, tidyverse)\n\nformat_range <- function(x){\n    paste0(range(x)[[1]], ' to ', range(x)[[2]])\n}\n```\n:::\n\n\n# The Binomial Distribution\nBinomial distributions are rampant, and their initial simplicity can belie complexities that I will probe in this post.\n\n$$Binomial(k, p)$$\n$$k = trials\\ per\\ sample$$\n$$p = Pr(success\\ event\\ on\\ each\\ trial)$$\n\n$$Binomial(k=1e4,\\ p=.75)$$\n\nIt's worth emphasizing some vocabulary: \n\n* a **parameter** summarizes an underlying truth (usually unknowable except when performing simulations). E.g., My customers have a 75% probability of converting. \n\n* a **statistic** summarizes a sample of observable data. E.g., 74.2% of my customers converted last month. \n\n* in a sense, **parameters** generate samples that can be summarized by **statistics**, and the generation process has some interesting propperties.  \n\nI'll specify two parameters for a Binomial distribution: \n\n::: {.cell}\n\n```{.r .cell-code}\nK <- 1e3\nP <- 0.75\n```\n:::\n\n\nGiven these parameters, what range of stats might I expect in my samples from the distribution $Binomial(k=1e4,\\ p=0.75)$? \n\n### rbinom()\nA really simple way to address this question is with `rbinom()`.\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\n\nrbinom(n=1, size=K, prob=P)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 760\n```\n:::\n:::\n\n\nIf I run the same code again I will get a different sample.\n\n::: {.cell}\n\n```{.r .cell-code}\nrbinom(n=1, size=K, prob=P)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 738\n```\n:::\n:::\n\n\nI can also run this as a series of Bernoulli trials\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(45)\n\nmy_bernoulli <- rbinom(n=K, size=1, prob=P)\n\nmy_bernoulli[1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 1 1 1 1 1 1 1 1 1 1\n```\n:::\n:::\n\n\nThe Bernoulli is a special case of the Binomial ($Bernoulli(p=0.75) = Binomial(k=1,\\ p=0.75)$), and the sum of the $k$ Bernoulli trials would represent the outcome for one Binomial sample with $k$ trials. \n\n::: {.cell}\n\n```{.r .cell-code}\nsum(my_bernoulli)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 771\n```\n:::\n:::\n\n\nThere are enormous efficiencies to be gained from using Binomial distribution rather than the Bernoulli when simulating thousands or millions of samples. But know that the results would converge. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nN <- 4e3 # samples to simulate\n```\n:::\n\n\nSo now I will simulate 4000 samples from $Binomial(k=1e4, p=0.75)$\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_events <- rbinom(n=N, size=K, prob=P)\n```\n:::\n\n\nrbinom() just outputs a long vector of event tallies. Let's look at 10. \n\n::: {.cell}\n\n```{.r .cell-code}\nsim_events[1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 740 717 736 765 774 756 769 705 727 735\n```\n:::\n:::\n\n\nThe full vector of sample stats is easier to visualize in a plot\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim_events) |> \n    ggplot(aes(x = sim_events)) + \n    geom_histogram(bins=200)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nSo we can see that the distribution $Binomial(1e4, 0.75)$ generated samples with 699 to 813 events. We could describe this range of potential samples further:\n\n::: {.cell}\n\n```{.r .cell-code}\npaste0('mean = ', mean(sim_events) |> round(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"mean = 750.124\"\n```\n:::\n\n```{.r .cell-code}\npaste0('SEM = ', sd(sim_events) |> round(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"SEM = 13.578\"\n```\n:::\n\n```{.r .cell-code}\nquantile(sim_events, c(0.025, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 2.5% 97.5% \n  724   776 \n```\n:::\n:::\n\n\nSo `rbinom()` will output a tally of events, but often we are more interested in the rate this represents.  \n\n::: {.cell}\n\n```{.r .cell-code}\nsim_rates <- sim_events / K\n\npaste0('mean = ', mean(sim_rates)  |> round(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"mean = 0.75\"\n```\n:::\n\n```{.r .cell-code}\npaste0('SEM = ', sd(sim_rates) |> round(5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"SEM = 0.01358\"\n```\n:::\n\n```{.r .cell-code}\nquantile(sim_rates, c(0.025, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 2.5% 97.5% \n0.724 0.776 \n```\n:::\n:::\n\n\nYou should note above that I have computed the Standard Error of the Mean using `sd(sim_rates)` because `sim_rates` actually represents a *sampling distribution*. `sim_rates` is a sample of samples ... and so the Central Limit Theorum kicks in to ensure \n\n* it approximates normality regardless that the raw data is not normal (the raw data are 0s and 1s), \n\n* its mean approximates the true parameter mean of 0.75\n\n* the standard deviation of the sampling distribution is the standard error of the mean\n\nNormally you would have tried to mathematically estimate such results from just one real sample: \n\n::: {.cell}\n\n```{.r .cell-code}\n# grab just one random sample\nrate1 <- sim_rates[[18]]\n\n# std dev of binomial data is sqrt(p*(1-p))\nsd1 <- sqrt(rate1 * (1-rate1))\n\n# convert std dev to SEM\nse1 <- sd1 / sqrt(K)\n\nse1 |> round(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.01401\n```\n:::\n:::\n\n\nThat's pretty cool how close we can estimate the standard error with just one sample. \n\n## Summary: \nSo we've just simulated some data using `rbinom()` and explored its properties. \n\n# dbinom()\n`rbinom()` can generate simulated data and I find its results to be very intuitive. Density (`dbinom()`) eluded my intuition for an embarrassingly long time. Until I saw how the results converged with `rbinom()`.\n\nUsing the same distribution $Binomial(1e4, 0.75)$, let's first consider the range of stats that we could observe. In 1,000 trials I can only observe between 0 & 1,000 events, so this defines my range of observable stats. \n\n::: {.cell}\n\n```{.r .cell-code}\nrange_of_event_tallies <- 0:K\n```\n:::\n\n\nOf course, some of those stats are more likely to manifest in a sample than others. If I have a true underlying success rate of 75% this could more readily manifest as 748 events than it could manifest as 992 events in a sample of 1000 trials. 748 events certainly feels more likely, even though 992 events is not impossible. \n\nWe can use the `dbinom()` function to return the likelihood of each possible event tally from $Binomial(1e4, 0.75)$.\n\n::: {.cell}\n\n```{.r .cell-code}\nlikelihood_of_event_tallies <- \n    dbinom(\n        range_of_event_tallies,\n        size = K,\n        prob = P\n    )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_density <- \n    tibble(\n        range_of_event_tallies,\n        likelihood_of_event_tallies\n    )\n\ndf_density |> \n    ggplot(aes(x = range_of_event_tallies, y = likelihood_of_event_tallies)) + \n    geom_line() + \n    labs(\n        title = str_wrap(glue('Samples from Binomial({K}, {P}) cover a pretty narrow range centered around {K*P} events'), 65)\n    )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nAs we said earlier, it's usually easier to think about these stats in terms of rates rather than tallies, so we'll just divide the event_tallies by the trial count (`K`).\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_density <- \n    df_density |> \n    mutate(range_of_sample_rates = range_of_event_tallies / K)\n\np_rate <- \n    df_density |> \n    ggplot(aes(x = range_of_sample_rates, y = likelihood_of_event_tallies)) + \n    geom_line() + \n    scale_x_continuous(labels = percent) + \n    labs(\n        title = str_wrap(glue('Samples from Binomial({K}, {P}) cover a pretty narrow range centered around {percent(P)}'), 65)\n    )\n\np_rate\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\np_rate + \n    coord_cartesian(xlim = c(.7, .8)) + \n    labs(title = str_wrap(\"Zooming in I can eyeball that about 95% of potential samples will fall between 72.5% & 77.5%\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nBut don't take my word for it. Let's see where the 2.5% and 97.5%-iles of this distribution fall. \n\n::: {.cell}\n\n```{.r .cell-code}\nci95_events <- qbinom(c(0.025, 0.975), K, P)\n\nci95_rates <- ci95_events / K\n\nci95_rates\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.723 0.777\n```\n:::\n:::\n\n\nNot bad.\n\nNow for the kicker: We can arrive at nearly these same values through simulation or density functions. Let's revisit our simulated sampling distribution `sim_rates`:\n\n::: {.cell}\n\n```{.r .cell-code}\nquantile(sim_rates, c(0.025, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 2.5% 97.5% \n0.724 0.776 \n```\n:::\n:::\n\n\nTell me you don't have goose bumps!\n\n# All roads lead to Rome:\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_sim <- \n    tibble(range_of_sample_rates = sim_rates) |> \n    count(range_of_sample_rates) |> \n    mutate(prop = n / sum(n))\n\n# this join works very well to create a double y-axis\nleft_join(df_density, df_sim, by = 'range_of_sample_rates') |> \n    ggplot(aes(x = range_of_sample_rates, y = likelihood_of_event_tallies)) + \n    geom_col(aes(x=range_of_sample_rates, y=prop)) + \n    geom_line(color = 'red', linewidth=1) + \n    scale_x_continuous(\n        breaks = pretty_breaks(10),\n        labels = ~percent(.x,1)\n        ) + \n    labs(\n        x = \"Potential Sample Rates\",\n        y = \"Likelihood of Sample Rate\",\n        title = str_wrap(\"Note the convergence of rbinom()'s histogram and dbinom()'s line\"), 65) + \n    coord_cartesian(xlim = c(.7,.8))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}