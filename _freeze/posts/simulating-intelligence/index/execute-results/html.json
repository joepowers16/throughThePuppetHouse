{
  "hash": "fca5e924dfef7a4862693415c51c299e",
  "result": {
    "markdown": "---\ntitle: \"Simulating Intelligence\"\nauthor: \"Joseph Powers\"\ndate: \"2023-12-16\"\ncategories: [simulation, learning, data simulation, modeling]\nimage: \"image.png\"\ndraft: false\n---\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(glue, scales, tidyverse)\n```\n:::\n\n\n# A million dollar beer\nMy career took a turn for the better about 5 years ago when I asked a friend who was much stronger in statistics how he got so good. \"Data simulation!\" he pronounced over his beer. \"I simulate data with known relationships and when I can reliably uncover the relationship that I put there ... then I trust the new modeling technique. I hardly use any math at the early stages of learning.\" I am happy to confirm that this method works really, really well. However, data simulation is rarely taught in a standard statistics course, and so I will offer a primer in this blog. \n\n# The likelihood functions are your friends\nYou probably saw many a stack overflow post or textbook example start off with `rnorm()` or `rbinom()` and then jump right into a higher level data issue without ever properly introducing you to these wunderkinds. Allow me to offer a proper introduction. \n\nImagine you have the following scenario\n\n::: {.cell}\n\n```{.r .cell-code}\nCUSTOMERS <- 1e5\nBASE_RATE <- 0.5\n```\n:::\n\n\nYou have 100,000 customers and historically 50% of them do something that you care about. Maybe it's a click rate or a graduation rate or show up on time for an appointment. We all care about different things. Pick one that matters to you. \n\nI work in tech so I care about conversion rates, and I'll anchor to that. So imagine that historically I see 100,000 new potential customers show up in December and 50% of them convert. But we also know that such point estimates have variance in the real world. We can represent this variance with likelihood functions. \n\nQuick vocab: each customer is a `trial`, each conversion is an `event`, and the binomial function outputs a tally of events within a `sample` or `set of trials`. The term \"sample\" can refer to an individual trialer or  to a group of trialers. In this post, \"sample\" only refers to the group.\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42) # ensures reproducibility of simulations\n\nevents <- \n    rbinom(\n        n = 1, # samples (where 1 sample = 1 set of trials)\n        size = 10, # trials per sample\n        prob = .5 # Pr(event) in each trial\n    )\n\nevents\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 7\n```\n:::\n:::\n\n\nIn the above example `rbinom()` simulated 7 events from 1 set of 10 trials. This could represent a sample of \"7 heads on 10 flips\" or \"7 conversions among 10 website visitors.\" But this is just one sample. \n\nWe can also simulate multiple samples:\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42) # ensures reproducibility of simulations\n\nevents_20 <- \n    rbinom(\n        n = 20, # samples (where 1 sample = 1 set of trials)\n        size = 10, # trials per sample\n        prob = .5 # Pr(event) in each trial\n    )\n\nevents_20\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 7 7 4 7 6 5 6 3 6 6 5 6 7 4 5 7 8 3 5 5\n```\n:::\n:::\n\n\nAs the number of samples grows it becomes more practical to represent the outcomes visually. \n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42) # ensures reproducibility of simulations\n\ndf_1e5 <- \n    tibble(\n        events = \n            rbinom(\n                n = 1e5, # samples (where 1 sample = 1 set of trials)\n                size = 10, # trials per sample\n                prob = .5 # Pr(event) in each trial\n            )\n        )\n\ndf_1e5 |> \n    ggplot(aes(x = events)) + \n    geom_histogram(bins=200) + \n    scale_x_continuous(breaks = 1:10) + \n    labs(\n        title = glue(\"{comma(1e5)} simulated samples from Binomial(10, 0.5)\"), \n        x = \"Simulated Event Tallies\") \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nWhat does this mean? Well, one eye-opening insight for me was that this simulated distribution represents the range and frequency of possible samples that could manifest in 10 trials with a 50% probability of success.\n\nThere are other ways to represent this information, for instance we can convert these event tallies back into rates, which is often more intuitive.\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_1e5 |> \n    mutate(rate = events / 10) |> \n    ggplot(aes(x = rate)) + \n    geom_histogram(bins=200) + \n    scale_x_continuous(\n        breaks = pretty_breaks(10),\n        labels = percent) + \n    labs(title = str_wrap(glue(\"Think of this plot as representing the range of observable samples from Binomial(k=10, p=0.5)\")), x = glue(\"Observed Success Rates\\nin 100k Simulated Samples from Binomial(k=10, p=0.5)\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42) # ensures reproducibility of simulations\n\nK <- 30\n\ndf_sim_k30_n1e5 <- \n    tibble(\n        events = \n            rbinom(\n                n = 1e5, # samples (where 1 sample = 1 set of trials)\n                size = K, # trials per sample\n                prob = .5 # Pr(event) in each trial\n            ),\n        rate = events / K\n        )\n\ndf_sim_k30_n1e5 |> \n    ggplot(aes(x = events)) + \n    geom_histogram(bins=K+10) + \n    scale_x_continuous(breaks = 1:K) + \n    labs(\n        title = glue(\"{comma(1e5)} simulated samples from Binomial({K}, 0.5)\"), \n        x = \"Simulated Event Tallies\") \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\ndf_sim_k30_n1e5 |> \n    ggplot(aes(x = rate)) + \n    geom_histogram(binwidth = .01) + \n    scale_x_continuous(\n        breaks = pretty_breaks(10),\n        label = percent) + \n    labs(\n        title = glue(\"{comma(1e5)} simulated samples from Binomial({K}, 0.5)\"), \n        \"Observed Rates in Simulated Samples\") \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42) # ensures reproducibility of simulations\n\nK <- 1e3\n\ndf_sim_k1e3_n1e5 <- \n    tibble(\n        events = \n            rbinom(\n                n = 1e5, # samples (where 1 sample = 1 set of trials)\n                size = K, # trials per sample\n                prob = .5 # Pr(event) in each trial\n            ),\n        potential_sample_rates = events / K\n        )\n\ndf_sim_k1e3_n1e5 |> \n    ggplot(aes(x = events)) + \n    geom_histogram(bins=K+10) + \n    scale_x_continuous(breaks = 1:K) + \n    labs(\n        title = glue(\"{comma(1e5)} simulated samples from Binomial({K}, 0.5)\"), \n        x = \"Simulated Event Tallies\") \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\ndf_sim_k1e3_n1e5 |> \n    ggplot(aes(x = potential_sample_rates)) + \n    geom_histogram(bins=200) + \n    scale_x_continuous(\n        breaks = pretty_breaks(10),\n        label = percent) + \n    labs(\n        title = glue(\"{comma(1e5)} simulated samples from Binomial({K}, 0.5)\"), \n        x = \"Observed Rates in Simulated Samples\") \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n:::\n\n\nI think by now you're starting to see the Central Limit Theorum kick in. \n\n::: {.cell}\n\n```{.r .cell-code}\ndf_dens_k1e3_n1e5 <- \n    tibble(\n        potential_sample_events = seq(0,K,1),\n        likelihood = dbinom(potential_sample_events, K, BASE_RATE),\n        potential_sample_rates = potential_sample_events / K\n    )\n\ndf_dens_k1e3_n1e5 |> \n    ggplot(aes(x = potential_sample_events, y = likelihood)) + \n    geom_line() + \n    scale_x_continuous(\n        breaks = \n    ) + \n    coord_cartesian(xlim = c(430, 570))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_dens_k1e3_n1e5 |> \n    ggplot(aes(x = potential_sample_rates, y = likelihood)) + \n    geom_line() + \n    scale_x_continuous(\n        breaks = pretty_breaks(10),\n        labels = percent\n    ) + \n    coord_cartesian(xlim = c(.430, .570)) + \n    labs(y = \"likelihood of observing any rate in a sample\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nWORKING HERE: \nI was hoping to overlay these plots\n\n# Demo pbinom() vs rbinom()\n\n::: {.cell}\n\n```{.r .cell-code}\npbinom(.5*K, size = K, prob = BASE_RATE) # P[X≤x]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5126125\n```\n:::\n:::\n\n\nPretty close...\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(df_sim_k1e3_n1e5$events <= .5*K)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.51474\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nqbinom(.025, size = K, prob = BASE_RATE) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 469\n```\n:::\n\n```{.r .cell-code}\nqbinom(.975, size = K, prob = BASE_RATE) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 531\n```\n:::\n:::\n\n\nDead match!\n\n::: {.cell}\n\n```{.r .cell-code}\nquantile(df_sim_k1e3_n1e5$events, c(.025, .975))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 2.5% 97.5% \n  469   531 \n```\n:::\n:::\n\n\n# Overlay plots\n\n::: {.cell}\n\n```{.r .cell-code}\ndemo_K <- 100\n\ndf1 <- tibble(\n    x = seq(0,demo_K,1),\n    y = dbinom(x, demo_K, .5)\n)\n\ndf2 <- tibble(\n    x = rbinom(1e5, demo_K, .5)\n) |> \n    count(x) |> \n    mutate(\n        prop = n / sum(n)\n    )\n\n# this join works very well to create a double y-axis\nleft_join(df1, df2, by = 'x') |> \n    ggplot(aes(x = x, y = y)) + \n    geom_col(aes(x=x, y=prop)) + \n    geom_line(color = 'red') + \n    scale_x_continuous(\n        breaks = pretty_breaks(10),\n        labels = ~percent(.x/demo_K)\n        ) + \n    scale_y_continuous(\n        sec.axis = \n            sec_axis(\n                trans = ~.x*demo_K, \n                name = \"Tally of Simulated Samples\"\n            ) \n    ) + \n    labs(\n        x = \"Potential Sample Rates\",\n        y = \"Likelihood of Sample Rate\",\n        title = str_wrap(\"Note the convergence of rbinom()'s histogram and dbinom()'s line\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The `trans` argument of `sec_axis()` is deprecated as of ggplot2 3.5.0.\nℹ Please use the `transform` argument instead.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 58 rows containing missing values or values outside the scale range\n(`geom_col()`).\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nNote how the likelihood density values from a discrete probability distribution like the binomnial are highly interpretable. For instance there is literally an 8% probability that $Binomial(k=100,p=8)$ will manifest a sample rate of 50%. \n\nWhen we dive into the normal distribution, you will see that the likelihood \n\n[image source](https://www.ebay.com/itm/354691543180?chn=ps&_trkparms=ispr%3D1&amdata=enc%3A19eTrIrJaTCiDc8pmNv6uQA60&norover=1&mkevt=1&mkrid=711-117182-37290-0&mkcid=2&mkscid=101&itemid=354691543180&targetid=1585159291611&device=c&mktype=pla&googleloc=9061203&poi=&campaignid=19894961968&mkgroupid=148855406073&rlsatarget=pla-1585159291611&abcId=9307911&merchantid=6296724&gclid=Cj0KCQiAsvWrBhC0ARIsAO4E6f8aQjDQSMq4XWczbr1gPwTiN9s68V3PCf5P8baT1yNMfRNQdNUqigkaAj8wEALw_wcB)",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}