[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is a place for me to share and explore topics of interest, mostly stats and programming … maybe gardening and woodworking when I get ambitious."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Through the Puppet House",
    "section": "",
    "text": "The Difference between Probability and Likelihood\n\n\n\n\n\n\nbayesian\n\n\nhlm\n\n\nmlm\n\n\nmodeling\n\n\n\n\n\n\n\n\n\nNov 24, 2024\n\n\nJoseph Powers\n\n\n\n\n\n\n\n\n\n\n\n\nLinearity in Logistic Regression Models\n\n\n\n\n\n\nmodeling\n\n\nglm\n\n\nlogistic\n\n\n\n\n\n\n\n\n\nOct 21, 2024\n\n\nJoseph Powers\n\n\n\n\n\n\n\n\n\n\n\n\nGetting and Setting Priors in Bayesian Models with brms package\n\n\n\n\n\n\nbayesian\n\n\nhlm\n\n\nmlm\n\n\nmodeling\n\n\n\n\n\n\n\n\n\nApr 21, 2024\n\n\nJoseph Powers\n\n\n\n\n\n\n\n\n\n\n\n\nBinomial Deep Dive\n\n\n\n\n\n\nsimulation\n\n\nlearning\n\n\ndata simulation\n\n\nmodeling\n\n\nbinomial\n\n\n\n\n\n\n\n\n\nDec 16, 2023\n\n\nJoseph Powers\n\n\n\n\n\n\n\n\n\n\n\n\nSimulating Intelligence\n\n\n\n\n\n\nsimulation\n\n\nlearning\n\n\ndata simulation\n\n\nmodeling\n\n\n\n\n\n\n\n\n\nDec 16, 2023\n\n\nJoseph Powers\n\n\n\n\n\n\n\n\n\n\n\n\nBinomial Normal Convergence\n\n\n\n\n\n\nsimulation\n\n\nlearning\n\n\ndata simulation\n\n\nmodeling\n\n\nbinomial\n\n\n\n\n\n\n\n\n\nDec 16, 2023\n\n\nJoseph Powers\n\n\n\n\n\n\n\n\n\n\n\n\nWrite clearer functions in R using quasiquotation\n\n\n\n\n\n\nfunctions\n\n\nnonstandard evaluation\n\n\n\n\n\n\n\n\n\nNov 17, 2018\n\n\nJoseph Powers\n\n\n\n\n\n\n\n\n\n\n\n\nHerding cats with list columns and purrr\n\n\n\n\n\n\nlist columns\n\n\niteration\n\n\npurrr\n\n\n\n\n\n\n\n\n\nJul 15, 2018\n\n\nJoseph Powers\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/binomial-normal-convergence/index.html",
    "href": "posts/binomial-normal-convergence/index.html",
    "title": "Binomial Normal Convergence",
    "section": "",
    "text": "pacman::p_load(glue, scales, tidyverse)\n\nformat_range &lt;- function(x){\n    paste0(range(x)[[1]], ' to ', range(x)[[2]])\n}"
  },
  {
    "objectID": "posts/binomial-normal-convergence/index.html#summary",
    "href": "posts/binomial-normal-convergence/index.html#summary",
    "title": "Binomial Normal Convergence",
    "section": "Summary:",
    "text": "Summary:\nSo we’ve just simulated some data using rbinom() and explored its properties."
  },
  {
    "objectID": "posts/2024-11-24_probvlik/index.html",
    "href": "posts/2024-11-24_probvlik/index.html",
    "title": "The Difference between Probability and Likelihood",
    "section": "",
    "text": "TL;DR:\n\nBayes Rule: \\(Posterior\\ Probability \\propto Likelihood\\ *\\ Prior\\ Probability\\)\nThe Likelihood expresses the likelihood of different parameters for your sample of data.\nThe Prior Probability express your prior beliefs about the probability that each of those parameters could be truly present.\nThe Prior Probability is then conditioned on (AKA averaged with) the Likelihood of the Data to express your updated beliefs as a Posterior Probability distribution.\n\n\npacman::p_load(tidyverse, knitr, gt, scales, glue, brms, conflicted)\n\nconflicts_prefer(dplyr::filter)\nconflicts_prefer(dplyr::lag)\nconflicts_prefer(brms::ar)\nconflicts_prefer(scales::col_factor)\nconflicts_prefer(scales::discard)\n\ntheme_set(theme_bw())\n\nlor_to_prob &lt;- function(lor){exp(lor) / (1+exp(lor))}\nprob_to_lor &lt;- function(prob){log(prob / (1-prob))}\n\n\nset.seed(5)\nSIMS &lt;- 1\nPOST_SIMS &lt;- 4e3\nRATE_A &lt;- .65\nRATE_B &lt;- .66\nTRIALS &lt;- 10e3\nrecipes &lt;- c(\"A\", \"B\")\nlegend_levels &lt;- c(\"Prior Beliefs\", \"Likelihood of Sample\", \"Posterior Beliefs\")\n\ndf &lt;- \n    tibble(\n        recipe = recipes,\n        trials = rpois(length(recipes), TRIALS),\n        theta = c(RATE_A, RATE_B)\n    ) |&gt; \n    rowwise() |&gt;\n    mutate(events = rbinom(n=SIMS, size=trials, prob=theta)) |&gt; \n    ungroup() |&gt; \n    mutate(\n        rate = events/trials,\n        b_dtc = rate - lag(rate),\n        b_itc = rate/lag(rate))\n\ngt::gt(df) %&gt;%\n  fmt_number(\n    columns = c(rate, b_dtc, b_itc),\n    decimals = 3\n  )\n\n\n\n\n\n\n\nrecipe\ntrials\ntheta\nevents\nrate\nb_dtc\nb_itc\n\n\n\n\nA\n9915\n0.65\n6455\n0.651\nNA\nNA\n\n\nB\n9943\n0.66\n6712\n0.675\n0.024\n1.037\n\n\n\n\n\n\n\n\nX_LIMS &lt;- c(-0.02,.06)\nY_LIMS &lt;- c(0, 110)\n\np_pt &lt;- \n    df |&gt; \n    ggplot() + \n    geom_point(aes(x=b_dtc, y=0), size=2) +\n    # geom_label(aes(label = percent(df$b_dtc[[2]], .1))) + \n    coord_cartesian(xlim = X_LIMS, ylim = Y_LIMS) + \n    labs(x = \"Potential Parameters\", title = \"What you saw in your sample\") + \n    scale_x_continuous(\n        breaks = seq(-1,1,.01),\n        labels = percent\n    ) + \n    labs(y = NULL) + \n    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank())\n\np_pt\n\n\n\n\n\n\n\n\n\n\nWhat reality generated my sample?\nLet’s generate a 95% CI because 95% of CIs will contain the true parameter… which should sound worryingly different than what you practically want but what the hell, let’s see it.\n\np_se &lt;- p_pt + \n    geom_errorbarh(aes(y=0, xmin = ci_95[[1]], xmax = ci_95[[2]]), height=5) + \n    labs(\n        x = \"Potential Parameters\",\n        title = \"Mean Diff and 95% CI\", \n        subtitle = \"Are all values within that CI equally likely to be the parameter?\") + \n    labs(y = NULL) + \n    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank())\n\np_se\n\n\n\n\n\n\n\n\n\n\nHow often would each potential reality generate my sample?\n\ndf_dtc_lik &lt;- \n    tibble(\n        x = seq(X_LIMS[[1]], X_LIMS[[2]], length.out = POST_SIMS),\n        y = dnorm(x = B_DTC, mean = x, sd = SE_diff), \n        dist = \"Likelihood of Sample\"\n    )\n\np_se + \n    geom_line(data=df_dtc_lik , aes(x=x, y=y)) +\n    labs(\n        x = \"Potential Parameters for the Sample Delta\",\n        y = \"Likelihood that Parameter\\nGenerated the Sample Delta\"\n    )\n\n\n\n\n\n\n\n\nDo you believe that the tails of your 95% CI (1.1% vs 3.7%) are equally likely to be the parameter?” Of course not, in most business settings, an effect of 1.1% is 100x more likely to be true than an effect of 3.7%. So we need to incorporate this knowledge into the model that expresses our beliefs.\n\n# Express your prior beliefs\nPRIOR_MU_INTERCEPT &lt;- 0.64\nPRIOR_SD_INTERCEPT &lt;- 0.025\nPRIOR_SD_DTC &lt;- 0.005\n\n\ndf_prior_cont &lt;- tibble(\n    x = seq(-.02, .06, length.out = POST_SIMS),\n    y = dnorm(x, mean=0, sd=PRIOR_SD_DTC),\n    dist = \"Prior Beliefs\"\n)\n\np_prior &lt;- \n    df_prior_cont |&gt; \n    ggplot(aes(x=x, y=y)) + \n    geom_line(linetype = 'dashed') + \n    scale_x_continuous(\n        breaks = seq(-1,1,.01),\n        labels = percent\n    ) +\n    coord_cartesian(xlim=X_LIMS, ylim = Y_LIMS) + \n    labs(title = \"My Prior Beliefs about the probability of different treatment deltas\",\n        x = \"Probable Parameters\",\n        y = \"Prior Probability that\\neach Parameter is Real\"\n        ) + \n    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank())\n\np_prior\n\n\n\n\n\n\n\n\n\np_prior_pt &lt;- \n    p_prior + \n    geom_point(data=df, aes(x=b_dtc, y=0), size=2) + \n    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank())\n\np_prior_pt\n\n\n\n\n\n\n\n\n\nmy_formula &lt;- \"events | trials(trials) ~ 0 + Intercept + recipe\"\n\nget_prior(\n    my_formula,\n    family = binomial,\n    data = df\n)\n\n  prior class      coef group resp dpar nlpar lb ub       source\n (flat)     b                                            default\n (flat)     b Intercept                             (vectorized)\n (flat)     b   recipeB                             (vectorized)\n\n\n\n# Convert prior probabilities to log-odds ratios for logistic regression\nPRIOR_MU_INTERCEPT_LOR &lt;- prob_to_lor(PRIOR_MU_INTERCEPT)\nPRIOR_SD_INTERCEPT_LOR &lt;- prob_to_lor(PRIOR_MU_INTERCEPT + PRIOR_SD_INTERCEPT) - prob_to_lor(PRIOR_MU_INTERCEPT)\nPRIOR_SD_DTC_LOR &lt;- prob_to_lor(PRIOR_MU_INTERCEPT + PRIOR_SD_DTC) - prob_to_lor(PRIOR_MU_INTERCEPT)\n\nmy_priors &lt;- \n    prior(normal(PRIOR_MU_INTERCEPT_LOR, PRIOR_SD_INTERCEPT_LOR), class = b, coef = Intercept) + \n    prior(normal(0, PRIOR_SD_DTC_LOR), class = b, coef = recipeB)\n\nmy_stanvars &lt;- c(\n    stanvar(PRIOR_MU_INTERCEPT_LOR, name = \"PRIOR_MU_INTERCEPT_LOR\"),\n    stanvar(PRIOR_SD_INTERCEPT_LOR, name = \"PRIOR_SD_INTERCEPT_LOR\"),\n    stanvar(PRIOR_SD_DTC_LOR, name = \"PRIOR_SD_DTC_LOR\")\n)\n\nfit_informed &lt;- brm(\n    formula = my_formula,\n    data = df,\n    family = binomial,\n    prior = my_priors, \n    stanvars = my_stanvars, \n    cores = 4,\n    seed = 44,\n    file = 'fits/fit_informed.rds'\n)\n\n\ndraws_informed &lt;- as_draws_df(fit_informed)\n\nhead(draws_informed)\n\n# A draws_df: 6 iterations, 1 chains, and 4 variables\n  b_Intercept b_recipeB lprior  lp__\n1        0.67     0.029   2.92 -10.2\n2        0.62     0.055   0.97 -12.7\n3        0.69     0.035   2.41 -11.9\n4        0.66     0.029   3.05 -10.0\n5        0.66     0.040   2.25  -9.8\n6        0.65     0.026   3.26 -10.5\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n\n\n\ndf_post_informed &lt;- \n    draws_informed |&gt; \n    mutate(\n        post_dtc = lor_to_prob(b_Intercept + b_recipeB) - lor_to_prob(b_Intercept), \n        dist = \"Posterior Beliefs\",\n        dist = factor(dist, levels = legend_levels)\n    ) \n\nATE &lt;- median(df_post_informed$post_dtc)\nPR_LOSS &lt;- mean(df_post_informed$post_dtc &lt; 0)\nMEAN_LOSS_LOSS &lt;- mean(df_post_informed$post_dtc[df_post_informed$post_dtc &lt; 0])\nE_LOSS &lt;- PR_LOSS * MEAN_LOSS_LOSS\n\n\nmy_breaks &lt;- RATE_B-RATE_A\nmy_labels &lt;- glue(\"True Delta\\n= {percent(my_breaks, 1L)}\")\n\nmy_linetypes &lt;- c(\"dashed\", \"solid\", \"solid\")\nnames(my_linetypes) &lt;- c(\"Prior Beliefs\", \"Likelihood of Sample\", \"Posterior Beliefs\")\n\nmy_fill &lt;- c(\"red\", \"green\", \"lightblue\")\nnames(my_fill) &lt;- c(\"Prior Beliefs\", \"Likelihood of Sample\", \"Posterior Beliefs\")\n\nbind_rows(\n    df_dtc_lik, \n    df_prior_cont\n    ) |&gt; \n    mutate(dist = factor(dist, levels = legend_levels)) |&gt; \n    ggplot(aes(linetype = dist, fill=dist)) + \n    geom_line(aes(x=x, y=y)) + \n    geom_density(data=df_post_informed, aes(x=post_dtc), alpha=.75, color = \"lightblue\", alpha=.75) + \n    scale_linetype_manual(values = my_linetypes) +\n    scale_fill_manual(values = my_fill) + \n    scale_x_continuous(\n        breaks = seq(-1,1,.01),\n        labels = percent,\n        sec.axis = \n            dup_axis(\n                breaks = my_breaks,\n                labels = my_labels,\n                name = NULL\n            )\n    ) + \n    labs(\n        title = \"I can update my prior beliefs with the likelihood of my sample data\",\n        x = \"Potential Parameters\", y = \"Likelihood that Parameter is Real\", fill = \"Distribution\", linetype = \"Distribution\") + \n    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank())\n\n\n\n\n\n\n\n\n\nsessionInfo()\n\nR version 4.4.0 (2024-04-24)\nPlatform: x86_64-apple-darwin20\nRunning under: macOS Big Sur 11.7.10\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Los_Angeles\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] conflicted_1.2.0 brms_2.21.0      Rcpp_1.0.12      glue_1.8.0      \n [5] scales_1.3.0     gt_0.10.1        knitr_1.46       lubridate_1.9.3 \n [9] forcats_1.0.0    purrr_1.0.2      readr_2.1.5      tidyr_1.3.1     \n[13] tibble_3.2.1     ggplot2_3.5.1    tidyverse_2.0.0  dplyr_1.1.4     \n[17] stringr_1.5.1   \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1     farver_2.1.2         loo_2.7.0           \n [4] fastmap_1.2.0        tensorA_0.36.2.1     pacman_0.5.1        \n [7] digest_0.6.35        timechange_0.3.0     lifecycle_1.0.4     \n[10] StanHeaders_2.32.8   magrittr_2.0.3       posterior_1.5.0     \n[13] compiler_4.4.0       rlang_1.1.3          sass_0.4.9          \n[16] tools_4.4.0          utf8_1.2.4           yaml_2.3.8          \n[19] labeling_0.4.3       bridgesampling_1.1-2 htmlwidgets_1.6.4   \n[22] pkgbuild_1.4.4       curl_5.2.1           xml2_1.3.6          \n[25] abind_1.4-5          withr_3.0.0          grid_4.4.0          \n[28] stats4_4.4.0         fansi_1.0.6          colorspace_2.1-0    \n[31] inline_0.3.19        cli_3.6.3            mvtnorm_1.2-5       \n[34] rmarkdown_2.27       ragg_1.3.2           generics_0.1.3      \n[37] RcppParallel_5.1.7   rstudioapi_0.16.0    tzdb_0.4.0          \n[40] cachem_1.1.0         rstan_2.32.6         bayesplot_1.11.1    \n[43] parallel_4.4.0       matrixStats_1.3.0    vctrs_0.6.5         \n[46] V8_4.4.2             Matrix_1.7-0         jsonlite_1.8.8      \n[49] hms_1.1.3            systemfonts_1.1.0    codetools_0.2-20    \n[52] distributional_0.4.0 stringi_1.8.4        gtable_0.3.5        \n[55] QuickJSR_1.1.3       munsell_0.5.1        pillar_1.9.0        \n[58] htmltools_0.5.8.1    Brobdingnag_1.2-9    R6_2.5.1            \n[61] textshaping_0.4.0    evaluate_0.23        lattice_0.22-6      \n[64] backports_1.5.0      memoise_2.0.1        rstantools_2.4.0    \n[67] coda_0.19-4.1        gridExtra_2.3        nlme_3.1-164        \n[70] checkmate_2.3.1      xfun_0.44            pkgconfig_2.0.3"
  },
  {
    "objectID": "posts/2024-10-21_linearity/index.html",
    "href": "posts/2024-10-21_linearity/index.html",
    "title": "Linearity in Logistic Regression Models",
    "section": "",
    "text": "Code\npacman::p_load(tidyverse, glue, scales, patchwork, conflicted, gtsummary, broman)\n\nconflict_prefer(\"filter\", \"dplyr\")\n\ntheme_set(theme_bw())\n\nlogit &lt;- function(x) 1 / (1 + exp(-x))\n\nlog_odds_to_prob &lt;- function(log_odds) {\n  prob &lt;- exp(log_odds) / (1 + exp(log_odds))\n  return(prob)\n}\n\nprob_to_log_odds &lt;- function(prob) {\n  if (any(prob &lt;= 0 | prob &gt;= 1)) {\n    stop(\"Probabilities must be between 0 and 1 exclusively\")\n  }\n  log_odds &lt;- log(prob / (1 - prob))\n  return(log_odds)\n}"
  },
  {
    "objectID": "posts/2024-10-21_linearity/index.html#fitting-a-model",
    "href": "posts/2024-10-21_linearity/index.html#fitting-a-model",
    "title": "Linearity in Logistic Regression Models",
    "section": "Fitting a model",
    "text": "Fitting a model\n\nglm1 &lt;- glm(renew ~ tenure, family = \"binomial\", df_sim)\n\ngtsummary::tbl_regression(glm1, intercept = TRUE)\n\n\n\n\n\n\n\nCharacteristic\nlog(OR)1\n95% CI1\np-value\n\n\n\n\n(Intercept)\n-11\n-13, -9.5\n&lt;0.001\n\n\ntenure\n2.2\n1.9, 2.6\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\n\n\nCode\ndf_lor &lt;- gtsummary::tbl_regression(glm1, intercept = TRUE) |&gt; \n    as_tibble() |&gt; \n    janitor::clean_names()  \n   \ndf_lor |&gt; \n    separate(x95_percent_ci, sep = \",\", into = c(\"ci025\", \"ci975\")) |&gt; \n    rename(term = characteristic) |&gt; \n    mutate(\n        cum_lor = cumsum(log_or),\n        pred_prob = log_odds_to_prob(cum_lor) |&gt; percent(accuracy = .001)\n    )\n\n\n# A tibble: 2 × 7\n  term        log_or ci025 ci975   p_value cum_lor pred_prob\n  &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;    \n1 (Intercept) -11    -13   \" -9.5\" &lt;0.001    -11   0.002%   \n2 tenure      2.2    1.9   \" 2.6\"  &lt;0.001     -8.8 0.015%"
  },
  {
    "objectID": "posts/2024-10-21_linearity/index.html#fitting-a-centered-model",
    "href": "posts/2024-10-21_linearity/index.html#fitting-a-centered-model",
    "title": "Linearity in Logistic Regression Models",
    "section": "Fitting a centered model",
    "text": "Fitting a centered model\n\nglm_mc &lt;-\n    glm(renew ~ tenure_mc, \n        family = \"binomial\", \n        df_sim |&gt; mutate(tenure_mc = tenure - mean(tenure)))\n\ngtsummary::tbl_regression(glm_mc, intercept = TRUE)\n\n\n\n\n\n\n\nCharacteristic\nlog(OR)1\n95% CI1\np-value\n\n\n\n\n(Intercept)\n0.02\n-0.27, 0.32\n0.9\n\n\ntenure_mc\n2.2\n1.9, 2.6\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\n\n\nCode\nglm_mc_lor &lt;- \n    gtsummary::tbl_regression(glm_mc, intercept = TRUE) |&gt; \n    as_tibble() |&gt; \n    janitor::clean_names()  \n   \nglm_mc_lor |&gt; \n    separate(x95_percent_ci, sep = \",\", into = c(\"ci025\", \"ci975\")) |&gt; \n    rename(term = characteristic) |&gt; \n    mutate(\n        cum_lor = cumsum(log_or),\n        pred_prob = log_odds_to_prob(cum_lor) |&gt; percent(accuracy = .001)\n    )\n\n\n# A tibble: 2 × 7\n  term        log_or ci025 ci975   p_value cum_lor pred_prob\n  &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;    \n1 (Intercept) 0.02   -0.27 \" 0.32\" 0.9        0.02 50.500%  \n2 tenure_mc   2.2    1.9   \" 2.6\"  &lt;0.001     2.22 90.203%  \n\n\n\n\nCode\nmean(df_sim$tenure)\n\n\n[1] 4.972778\n\n\n\n\nCode\nB_int &lt;- coef(glm1)[1]\nB_tenure &lt;- coef(glm1)[2]\n\ndf_demo &lt;- tibble(\n    tenure = seq(0,10,.01), \n    log_odds_renew = B_int + B_tenure * tenure,\n    prob_renew = 1 / (1 + exp(-(B_int + B_tenure * tenure))),\n) \n\np_lor &lt;- df_demo |&gt; \n    ggplot(aes(x = tenure, y = log_odds_renew)) + \n    geom_line()\n\np_prob &lt;- df_demo |&gt; \n    ggplot(aes(x = tenure, y = prob_renew)) + \n    geom_line()\n\np_lor + p_prob\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_demo |&gt; \n    filter(between(tenure, 4, 7)) |&gt;  \n    pivot_longer(\n        -tenure,\n        names_to = \"Outcome Type\",\n        values_to = \"value\"\n    ) |&gt; \n    ggplot(aes(x = tenure, y = value, color = `Outcome Type`)) + \n    geom_line() + \n    scale_x_continuous(breaks = 0:10) + \n    scale_y_continuous(breaks = c(-3:5)) + \n    labs(title = str_wrap(\"Zooming in on tenure between 4-6 yrs to contrast probability and log odds values\", 65))\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_new &lt;- tibble(\n    tenure = seq(0,10,length.out=1e3),\n    pred_lor = coef(glm1)[\"(Intercept)\"] + coef(glm1)[\"tenure\"] * tenure,\n    pred_prob = logit(pred_lor)\n)\n\n\nThe line was fit while the data were transformed to a line via logit().\n\n\nCode\ndf_new |&gt; \n    pivot_longer(\n        -tenure,\n        names_to = \"outcome_format\",\n        values_to = 'outcome'\n    ) |&gt; \n    ggplot() + \n    geom_line(aes(x = tenure, y = outcome, color = outcome_format)) + \n    scale_x_continuous(breaks = 0:10) + \n    scale_y_continuous(\n        breaks = c(-10, -5, 0, 1, 5, 10)\n    ) + \n    labs(color = NULL, x = \"Customer Tenure (yrs)\", y = \"Predicted Outcome\") + \n    scale_color_discrete(labels = c(\"pred_lor\" = \"Log Odds\", \n                                  \"pred_prob\" = \"Probability\")) + \n    theme(panel.grid.minor.y = element_blank())\n\n\n\n\n\n\n\n\n\nThis logit line can be transformed to probability values for easier interpretation.\n\n\nCode\ndf_labs &lt;- df_new |&gt; \n    mutate(floor = floor(tenure)) |&gt; \n    group_by(floor) |&gt; \n    filter(tenure == min(tenure))\n\ndf_new |&gt; \n    ggplot() + \n    geom_jitter(data = df_sim, aes(x = tenure, y = renew, color = factor(renew)), alpha = .2, height = .05, show.legend = FALSE) + \n    geom_line(aes(x = tenure, y = pred_prob)) + \n    geom_label(data=df_labs, aes(x=tenure, y=pred_prob, label=percent(pred_prob,1L))) + \n    scale_x_continuous(breaks = 0:10) + \n    labs(color = NULL, x = \"Customer Tenure (yrs)\", y = \"Pr(Renew)\") + \n    scale_color_discrete(breaks = c(1,0)) + \n    scale_y_continuous(\n        labels = percent,\n        sec.axis = \n            dup_axis(\n                breaks = c(0,1),\n                labels = c(\"Cancel (0)\", \"Renew (1)\"),\n                name = \"Observed Outcome\"\n            )\n    )"
  },
  {
    "objectID": "posts/herding-cats/index.html",
    "href": "posts/herding-cats/index.html",
    "title": "Herding cats with list columns and purrr",
    "section": "",
    "text": "When I first learned about list columns, I thought, “Weird, when will I ever use these?” In the year since, list columns have become my go-to data storage method as soon as I feel like I’m herding cats. That feeling usually arises when my analysis moves from processing typical numeric and text data to iterative programming on more complex data objects like lists, subsets, models, and plots. All of which will fit very nicely within list columns.\nIn this post I’ll demonstrate the versatility of list columns to hold mulitple types of complex data as well as keep that data aligned so that you can iteratively program using the purrr package.\n\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(modelr) \nlibrary(knitr)\n\nYou’re probably already familiar with character, double, and integer vectors like those found in the mpg tibble below (Note: if you weren’t already familiar with tibbles, they are just a special type of dataframe that has been formatted to handle list columns).\n\nmpg %&gt;% head() \n\n# A tibble: 6 × 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n  &lt;chr&gt;        &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; \n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa…\n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa…\n3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa…\n4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa…\n5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p     compa…\n6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p     compa…\n\n\nList columns are another kind of vector. They’re like the suitcase from Incredible Beasts and Where to Find Them: They look small but whole worlds can exist inside. While atomic vectors (e.g., double and character vectors) store individual numeric or text values, list columns can store complex data types like models, functions, plots, or even dataframes within dataframes or tibbles within tibbles. In the example below I use group_by() and nest() to create a list column, which by default will be named data.\n\nds_mpg &lt;- \n  mpg %&gt;% \n    group_by(cyl) %&gt;% \n    nest() %&gt;% \n    arrange(cyl)\n\nds_mpg\n\n# A tibble: 4 × 2\n# Groups:   cyl [4]\n    cyl data              \n  &lt;int&gt; &lt;list&gt;            \n1     4 &lt;tibble [81 × 10]&gt;\n2     5 &lt;tibble [4 × 10]&gt; \n3     6 &lt;tibble [79 × 10]&gt;\n4     8 &lt;tibble [70 × 10]&gt;\n\n\nEach element of the list column data contains all the data associated with that cylinder count. Let’s view the first element.\n\nds_mpg$data[[1]] |&gt; head() \n\n# A tibble: 6 × 10\n  manufacturer model      displ  year trans      drv     cty   hwy fl    class  \n  &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  \n1 audi         a4           1.8  1999 auto(l5)   f        18    29 p     compact\n2 audi         a4           1.8  1999 manual(m5) f        21    29 p     compact\n3 audi         a4           2    2008 manual(m6) f        20    31 p     compact\n4 audi         a4           2    2008 auto(av)   f        21    30 p     compact\n5 audi         a4 quattro   1.8  1999 manual(m5) 4        18    26 p     compact\n6 audi         a4 quattro   1.8  1999 auto(l5)   4        16    25 p     compact\n\n\nAs you can see above the list column elements are entire tibbles, all lined up for iterative processing with for loops or–better still–purrr's map functions. In the next chunks I apply a custom function that fits a linear model to each tibble in the list column, and then saves the model output in yet another list column. This avoids repetition in code and maintains alignment of data, models, and outputs. Not bad.\n\n# Create a custom function for modeling the data\nfoo_model &lt;- function(data){\n  lm(hwy ~ displ, data)\n}\n\n# iteratively apply that function to each dataframe in the list column called data\nds_mpg %&lt;&gt;% \n  mutate(model = map(.x = data, .f = foo_model))\n\nds_mpg |&gt; head() \n\n# A tibble: 4 × 3\n# Groups:   cyl [4]\n    cyl data               model \n  &lt;int&gt; &lt;list&gt;             &lt;list&gt;\n1     4 &lt;tibble [81 × 10]&gt; &lt;lm&gt;  \n2     5 &lt;tibble [4 × 10]&gt;  &lt;lm&gt;  \n3     6 &lt;tibble [79 × 10]&gt; &lt;lm&gt;  \n4     8 &lt;tibble [70 × 10]&gt; &lt;lm&gt;  \n\n\nNow that I have the models saved in their own list column called model I can work iteratively with them as well. What if I want to plot the predicted values of each model? Well I can do so and save the plots in yet another list column.\nTo plot the model outputs I’ll want to create a data grid of all possible values and then use ggplot() to create a unique plot for each models predictions against its actual data.\n\n# First let's add another list column for the data grids\nds_mpg %&lt;&gt;%\n  mutate(\n    grid = map(\n      .x = data, # the variable you are mapping to ...\n      .f = data_grid, # ... the function\n        # next come arguments for the function \n        displ = seq_range(displ, 500)\n    )\n  ) \n\n# then let's add predicted values\nds_mpg %&lt;&gt;% \n  mutate(pred = map2(grid, model, add_predictions, var = \"hwy_pred\"))\n\nds_mpg |&gt; head() \n\n# A tibble: 4 × 5\n# Groups:   cyl [4]\n    cyl data               model  grid               pred              \n  &lt;int&gt; &lt;list&gt;             &lt;list&gt; &lt;list&gt;             &lt;list&gt;            \n1     4 &lt;tibble [81 × 10]&gt; &lt;lm&gt;   &lt;tibble [500 × 1]&gt; &lt;tibble [500 × 2]&gt;\n2     5 &lt;tibble [4 × 10]&gt;  &lt;lm&gt;   &lt;tibble [1 × 1]&gt;   &lt;tibble [1 × 2]&gt;  \n3     6 &lt;tibble [79 × 10]&gt; &lt;lm&gt;   &lt;tibble [500 × 1]&gt; &lt;tibble [500 × 2]&gt;\n4     8 &lt;tibble [70 × 10]&gt; &lt;lm&gt;   &lt;tibble [500 × 1]&gt; &lt;tibble [500 × 2]&gt;\n\n\nFinally let’s create plots for each cylinder count, add custom titles, and save the results in out last list column plots.\n\n# Create custom plotting function\ngg_foo &lt;- function(data_orig = data, data_pred = pred, cyl = cyl){\n  # generate plot title\n  plot_title &lt;- paste(cyl, \"cylinder cars\")\n    \n  # plot raw data\n  data_orig %&gt;% \n    ggplot(aes(x = displ, y = hwy)) + \n    geom_point(color = \"blue\") +\n    # plot model predicted values\n    geom_point(\n      data = data_pred, \n      aes(x = displ, y = hwy_pred), \n      color = \"red\",\n      size = .5\n    ) + \n    scale_x_continuous(\n      breaks = seq(1, 7, by = 1),\n      limits = c(1, 7)\n    ) + \n    scale_y_continuous(\n      breaks = seq(10, 50, by = 10),\n      limits = c(10, 50)\n    ) +\n    labs(\n      title = plot_title, \n      x = \"Engine Displacement (L)\",\n      y = \"Hwy MPG\"\n    ) +\n    theme_minimal()\n}\n\n# Iteratively apply function \nds_mpg %&lt;&gt;%\n  mutate(\n    plots = \n      pmap(       \n        .l = list(\n          data_orig = data, \n          data_pred = pred, \n          cyl = cyl\n        ), \n        .f = gg_foo\n      )\n  ) \n\nds_mpg |&gt; head() \n\n# A tibble: 4 × 6\n# Groups:   cyl [4]\n    cyl data               model  grid               pred               plots \n  &lt;int&gt; &lt;list&gt;             &lt;list&gt; &lt;list&gt;             &lt;list&gt;             &lt;list&gt;\n1     4 &lt;tibble [81 × 10]&gt; &lt;lm&gt;   &lt;tibble [500 × 1]&gt; &lt;tibble [500 × 2]&gt; &lt;gg&gt;  \n2     5 &lt;tibble [4 × 10]&gt;  &lt;lm&gt;   &lt;tibble [1 × 1]&gt;   &lt;tibble [1 × 2]&gt;   &lt;gg&gt;  \n3     6 &lt;tibble [79 × 10]&gt; &lt;lm&gt;   &lt;tibble [500 × 1]&gt; &lt;tibble [500 × 2]&gt; &lt;gg&gt;  \n4     8 &lt;tibble [70 × 10]&gt; &lt;lm&gt;   &lt;tibble [500 × 1]&gt; &lt;tibble [500 × 2]&gt; &lt;gg&gt;  \n\n\nYou can preview the plots just by calling the plots variable:\n\nds_mpg$plots\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\nYou can also add file names and save the plots.\n\nname_plots &lt;- function(name = cyl){\n  str_c(\"plot_\", name, \"cyl.png\")\n}\n\nds_mpg %&lt;&gt;% \n  mutate(file_names = map_chr(cyl, name_plots))\n\n\nmap2(\n  ds_mpg$file_names, \n  ds_mpg$plots, \n  ggsave, \n   height = 4,\n   width = 6\n)\n\nThis approach might seem overkill for a small dataset like mpg, but imagine if you had a larger dataset with dozens or hundreds of groups you wanted to process. In such scenarios, list columns can be an invaluable tool for manipulating and storing complex data objects by herding them into a familiar data structure, the dataframe/tibble.\n\nReferences\nMake ggplot2 purrr"
  },
  {
    "objectID": "posts/setting_priors/index.html",
    "href": "posts/setting_priors/index.html",
    "title": "Getting and Setting Priors in Bayesian Models with brms package",
    "section": "",
    "text": "Let’s pretend we have data from a “typical” AB test of a rate-based metrics. Could be conversions, clicks, etc., doesn’t matter.\n\n\nCode\npacman::p_load(tidybayes, brms, glue, scales, tidyverse, truncnorm, patchwork, conflicted)\n\nconflicted::conflict_prefer(\"filter\", \"dplyr\")\n\ntheme_set(theme_bw())\n\nlor_to_prob &lt;- function(lor){exp(lor) / (1+exp(lor))}\nprob_to_lor &lt;- function(prob){log(prob / (1-prob))}\n\n# prob_to_lor(.5)\n# lor_to_prob(0)\n\n\n\n\nCode\nSIMS &lt;- 4e3\nTRIALS &lt;- 10e3\nRATE_A &lt;- .65\nRATE_B &lt;- .66\nTHETA_B &lt;- RATE_B - RATE_A\n\nset.seed(41) # 1.5%\n\ndf_wide &lt;- \n    tibble(\n        a = rbinom(TRIALS, 1, RATE_A),\n        b = rbinom(TRIALS, 1, RATE_B)\n    )\n\ndf_long &lt;- \n    df_wide |&gt; \n    pivot_longer(\n        everything(), \n        names_to = 'recipe',\n        values_to = 'outcome'\n    )\n\n# head(df_long)\n\n\n\n\nCode\ndf_long |&gt; \n    summarise(rate = mean(outcome), .by = recipe) |&gt; \n    ggplot(aes(x = recipe, y = rate, label = percent(rate, .1))) + \n    geom_col() + \n    geom_label() + \n    scale_y_continuous(labels = percent)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_agg &lt;- \n    df_wide |&gt; \n    summarise(\n        a = mean(a),\n        b = mean(b)\n    ) |&gt; \n    mutate(\n        b_dtc = b-a,\n        b_itc = b/a\n    )\n\ndf_agg\n\n\n# A tibble: 1 × 4\n      a     b  b_dtc b_itc\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 0.643 0.657 0.0138  1.02\n\n\n\n\nCode\nB_DTC &lt;- df_agg$b_dtc\n\n\nWe have observed that B is outperforming A by 1.38% points.\n\n\nCode\ndf_agg |&gt; \n    ggplot(aes(x = b_dtc, y = 1)) + \n    geom_vline(xintercept = B_DTC, linetype = 'dotted') + \n    geom_point(pch = 21, color = 'black', fill = 'blue', size=2) + \n    coord_cartesian(\n        xlim = c(-.02, .04),\n        ylim = c(0, 10)\n    ) + \n    scale_x_continuous(\n        breaks = seq(-.02, .04, .01), \n        labels = ~percent(.x, .01),\n        sec.axis = dup_axis(\n            breaks = B_DTC,\n            name = \"Observed Treatment Effect\"\n        )\n    ) + \n    labs(x = \"Potential Treatment Effects\", y = NULL) + \n    theme(\n        panel.grid = element_blank(),\n        axis.ticks.y = element_blank(), axis.text.y = element_blank())\n\n\n\n\n\n\n\n\n\nWhat can we say about this 1.38% treatment effect? Maybe we could explore the range of real effects that are likely to have produced our observed effect:\n\n\nCode\nset.seed(42)\n\ndf_post &lt;- \n    tibble(\n        post_a = rbinom(SIMS, TRIALS, df_agg$a) / TRIALS,\n        post_b = rbinom(SIMS, TRIALS, df_agg$b) / TRIALS,\n        post_b_dtc = post_b - post_a\n    )\n\nci95 &lt;- quantile(df_post$post_b_dtc, c(.025, .975))\n\ndf_post |&gt; \n    ggplot(aes(x = post_b_dtc)) + \n    geom_density(fill='blue', alpha = .5) + \n    coord_cartesian(\n        xlim = c(-.02, .04),\n    ) +\n    scale_x_continuous(\n        breaks = seq(-.02, .04, .01), \n        labels = ~percent(.x, .01),\n        sec.axis = dup_axis(\n            breaks = B_DTC,\n            name = \"Observed Treatment Effect\"\n        )\n    ) + \n    labs(x = \"Plausible Real Treatment Effects\", y = NULL) + \n    theme(\n        panel.grid = element_blank(),\n        axis.ticks.y = element_blank(), axis.text.y = element_blank()\n        )\n\n\n\n\n\n\n\n\n\nSo we can see that the range of plausible treatment effects extends from about -2% to 4%, and we can eyeball that the 90% CI would extend from about 0.25 to 2.5%, which would mean that 0.25% is about as likely as 2.5% to be the true treatment effect. Pause, do you really believe that 2.5% is just as plausible as 0.25% to be the true effect? I don’t. In my experience 0.25% point effect are much more likely that 2.5% points effects.\nWe can and should leverage such knowledge about what effect sizes are likley by supplying our model with informed priors.\n\n\nCode\ndf_agg_glm &lt;- \n    df_long |&gt; \n    summarise(\n        events = sum(outcome),\n        trials = length(outcome),\n        .by = recipe\n    )\n\n\n\nFit Bayesian Models\nUse get_prior() to see which priors need to be supplied.\n\n\nCode\nmy_formula &lt;- \"events | trials(trials) ~ recipe\"\n\nbrms::get_prior(\n    formula = my_formula,\n    family = binomial,\n    data = df_agg_glm\n)\n\n\n                prior     class    coef group resp dpar nlpar lb ub\n               (flat)         b                                    \n               (flat)         b recipeb                            \n student_t(3, 0, 2.5) Intercept                                    \n       source\n      default\n (vectorized)\n      default\n\n\n\n\nFit a Model with Uninformed Priors\nSince we’re predicting a binary outcome we’ll be using logistic regression, which will require our priors to be expressed in log odds ratios.\nI often demonstrate uninformed priors in log odds of uniform(-5, 5) as this represents a belief that all rates between 1% and 99% are equally likely. It supplies practically no new information to the model.\n\n\nCode\nlor_to_prob(5)\n\n\n[1] 0.9933071\n\n\nCode\nlor_to_prob(-5)\n\n\n[1] 0.006692851\n\n\n\n\nCode\nfit_uninformed &lt;- brm(\n    formula = my_formula,\n    data = df_agg_glm,\n    family = binomial,\n    prior = \n        prior(uniform(-5, 5), class = Intercept) + \n        prior(uniform(-5, 5), class = b, coef = recipeb), \n    cores = 4,\n    seed = 44,\n    file = 'fits/fit_uninformed.rds'\n)\n\n\nExtract posterior draws:\n\n\nCode\ndraws_uninformed &lt;- as_draws_df(fit_uninformed)\n\nhead(draws_uninformed)\n\n\n# A draws_df: 6 iterations, 1 chains, and 4 variables\n  b_Intercept b_recipeb lprior lp__\n1        0.54     0.081   -4.6  -18\n2        0.56     0.101   -4.6  -15\n3        0.56     0.118   -4.6  -16\n4        0.59     0.054   -4.6  -14\n5        0.60     0.053   -4.6  -14\n6        0.58     0.065   -4.6  -14\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n\n\n\n\nCode\ndf_post_uninformed &lt;- \n    draws_uninformed |&gt; \n    mutate(\n        b_pr = lor_to_prob(b_Intercept + b_recipeb) - lor_to_prob(b_Intercept)\n    ) \n\nPR_LOSS &lt;- mean(df_post_uninformed$b_pr &lt; 0)\nMEAN_LOSS_LOSS &lt;- mean(df_post_uninformed$b_pr[df_post_uninformed$b_pr &lt; 0])\nE_LOSS &lt;- PR_LOSS * MEAN_LOSS_LOSS\n\n\n\n\nCode\ndens &lt;- density(df_post_uninformed$b_pr)\n\ndf_fill_uninf &lt;- tibble(x = dens$x, y = dens$y)\n\nmy_breaks &lt;- median(df_post_uninformed$b_pr)\nmy_labels &lt;- percent(my_breaks)\n\ndf_fill_uninf |&gt;\n    ggplot(aes(x=x, y=y)) + \n    geom_line() + \n    geom_area(data = filter(df_fill_uninf, x &gt;= 0), fill = 'blue', alpha=.5) + \n    geom_area(data = filter(df_fill_uninf, x &lt; 0), fill = 'red', alpha=.5) + \n    scale_x_continuous(\n        breaks = seq(-1, 1, .01),\n        labels = ~percent(.x,.1),\n        sec.axis = \n            dup_axis(\n                breaks = my_breaks,\n                name = \"Median Posterior Treatment Effect\")) + \n    labs(y = NULL, x = \"Plausible Treatment Effects\", \n         title = glue(\"There is a {percent(PR_LOSS,.1)} probability that the real effect is harmful\"), \n         subtitle = glue(\"The expected loss is {percent(E_LOSS,.01)}\")) +  \n    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) \n\n\n\n\n\n\n\n\n\n\n\nFit a Model with Informed Priors\n\n\nCode\nprob_to_lor(.645)\n\n\n[1] 0.5971325\n\n\nCode\n(prob_to_lor(.645 + .05) - prob_to_lor(.645)) / 2\n\n\n[1] 0.1132338\n\n\nCode\n(prob_to_lor(.645 + .01) - prob_to_lor(.645)) / 2\n\n\n[1] 0.02197915\n\n\n\n\nCode\nfit_informed &lt;- brm(\n    formula = my_formula,\n    data = df_agg_glm,\n    family = binomial,\n    prior = \n        prior(normal(0.5971325, 0.1132338), class = Intercept) + \n        prior(normal(0, 0.02197915), class = b, coef = recipeb), \n    cores = 4,\n    seed = 44,\n    file = 'fits/fit_informed.rds'\n)\n\n\n\n\nCode\ndraws_informed &lt;- as_draws_df(fit_informed)\n\nhead(draws_informed)\n\n\n# A draws_df: 6 iterations, 1 chains, and 4 variables\n  b_Intercept b_recipeb lprior lp__\n1        0.59    0.0393    2.6 -7.4\n2        0.61    0.0049    4.1 -7.3\n3        0.60    0.0352    2.9 -7.1\n4        0.63    0.0171    3.8 -7.4\n5        0.60    0.0219    3.7 -6.9\n6        0.61    0.0193    3.7 -6.8\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n\n\n\n\nCode\ndf_post_informed &lt;- \n    draws_informed |&gt; \n    mutate(\n        b_pr = lor_to_prob(b_Intercept + b_recipeb) - lor_to_prob(b_Intercept)\n    ) \n\nPR_LOSS &lt;- mean(df_post_informed$b_pr &lt; 0)\nMEAN_LOSS_LOSS &lt;- mean(df_post_informed$b_pr[df_post_informed$b_pr &lt; 0])\nE_LOSS &lt;- PR_LOSS * MEAN_LOSS_LOSS\n\n\n\n\nCode\ndens &lt;- density(df_post_informed$b_pr)\n\ndf_fill_inf &lt;- tibble(x = dens$x, y = dens$y)\n\nmy_breaks &lt;- median(df_post_informed$b_pr)\nmy_labels &lt;- percent(my_breaks)\n\ndf_fill_inf |&gt;\n    ggplot(aes(x=x, y=y)) + \n    geom_line() + \n    geom_area(data = filter(df_fill_inf, x &gt;= 0), fill = 'blue', alpha=.5) + \n    geom_area(data = filter(df_fill_inf, x &lt; 0), fill = 'red', alpha=.5) + \n    scale_x_continuous(\n        breaks = seq(-1, 1, .01),\n        labels = ~percent(.x,.1),\n        sec.axis = \n            dup_axis(\n                breaks = my_breaks,\n                name = \"Median Posterior Treatment Effect\")) + \n    labs(y = NULL, x = \"Plausible Treatment Effects\", \n         title = glue(\"There is a {percent(PR_LOSS,.1)} probability that the real effect is harmful\"), \n         subtitle = glue(\"The expected loss is {percent(E_LOSS,.01)}\")) + \n    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) \n\n\n\n\n\n\n\n\n\n\n\nCode\nset.seed(42)\ndf_prior &lt;- tibble(x=rnorm(4e3, 0, .005))\ndens &lt;- density(df_prior$x)\ndf_prior &lt;- \n    tibble(\n        x = dens$x, \n        y = dens$y, \n        distribution = \"Prior of Normal(0, 0.5%)\")\n\n\nBelow you can see how the Posterior with Normal(0, 0.5%) Prior combines information from the likelihood of the raw data and the Normal(0, 0.5%) Prior.\n\n\nCode\nbind_rows(\ndf_fill_inf |&gt; mutate(distribution = 'Posterior with Normal(0, 0.5%) Prior'),\ndf_fill_uninf |&gt; mutate(distribution = 'Posterior with Uniform(1%, 99%) Prior')\n) |&gt; \n    ggplot(aes(x=x, y=y, group=distribution,  fill=distribution)) + \n    geom_line() + \n    geom_area(position = 'identity', alpha=.5) + \n    labs(x = \"Plausible Treatment Effects\", y = NULL) + \n    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) + \n    geom_line(\n        data = df_prior,\n        aes(x = x, y = y),\n        linetype = 'dashed'\n    ) + \n    scale_x_continuous(\n        breaks = seq(-1, 1, .01),\n        labels = ~percent(.x,.1),\n        sec.axis = \n            dup_axis(\n                breaks = THETA_B,\n                name = \"True Treatment Effect\"))\n\n\n\n\n\n\n\n\n\n\n\nCode\ndens &lt;- density(df_post$post_b_dtc)\n\ndf_likelihood &lt;- tibble(x = dens$x, y = dens$y)\n\nmy_values &lt;- c(1,1,2)\nnames(my_values) &lt;- c('Posterior with Normal(0, 0.5%) Prior', 'Likelihood of Raw Data', \"Prior of Normal(0, 0.5%)\")\n\nbind_rows(\n    df_fill_inf |&gt; mutate(distribution = 'Posterior with Normal(0, 0.5%) Prior', mylinetype = 1),\n    df_likelihood |&gt; mutate(distribution = 'Likelihood of Raw Data', mylinetype = 1)\n    ) |&gt; \n    ggplot(aes(x=x, y=y, group=distribution, fill=distribution, linetype = distribution)) + \n    geom_line(show.legend = FALSE) + \n    geom_area(position = 'identity', alpha=.5) + \n    labs(x = \"Plausible Treatment Effects\", y = NULL) + \n    theme(axis.ticks.y = element_blank(), axis.text.y = element_blank()) + \n    geom_line(data = df_prior) + \n    scale_x_continuous(\n        breaks = seq(-1, 1, .01),\n        labels = ~percent(.x, 1),\n        sec.axis = \n            dup_axis(\n                breaks = THETA_B,\n                name = \"True Treatment Effect\")) + \n    scale_linetype_manual(\n        values = my_values\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.0 (2024-04-24)\nPlatform: x86_64-apple-darwin20\nRunning under: macOS Big Sur 11.7.10\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Los_Angeles\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] conflicted_1.2.0 patchwork_1.2.0  truncnorm_1.0-9  lubridate_1.9.3 \n [5] forcats_1.0.0    purrr_1.0.2      readr_2.1.5      tidyr_1.3.1     \n [9] tibble_3.2.1     ggplot2_3.5.1    tidyverse_2.0.0  scales_1.3.0    \n[13] glue_1.8.0       brms_2.21.0      Rcpp_1.0.12      tidybayes_3.0.6 \n[17] dplyr_1.1.4      stringr_1.5.1   \n\nloaded via a namespace (and not attached):\n [1] svUnit_1.0.6         tidyselect_1.2.1     farver_2.1.2        \n [4] loo_2.7.0            fastmap_1.2.0        tensorA_0.36.2.1    \n [7] pacman_0.5.1         digest_0.6.35        timechange_0.3.0    \n[10] lifecycle_1.0.4      StanHeaders_2.32.8   magrittr_2.0.3      \n[13] posterior_1.5.0      compiler_4.4.0       rlang_1.1.3         \n[16] tools_4.4.0          utf8_1.2.4           yaml_2.3.8          \n[19] knitr_1.46           labeling_0.4.3       bridgesampling_1.1-2\n[22] htmlwidgets_1.6.4    pkgbuild_1.4.4       curl_5.2.1          \n[25] abind_1.4-5          withr_3.0.0          grid_4.4.0          \n[28] stats4_4.4.0         fansi_1.0.6          colorspace_2.1-0    \n[31] inline_0.3.19        cli_3.6.3            mvtnorm_1.2-5       \n[34] rmarkdown_2.27       ragg_1.3.2           generics_0.1.3      \n[37] RcppParallel_5.1.7   rstudioapi_0.16.0    tzdb_0.4.0          \n[40] cachem_1.1.0         rstan_2.32.6         bayesplot_1.11.1    \n[43] parallel_4.4.0       matrixStats_1.3.0    vctrs_0.6.5         \n[46] V8_4.4.2             Matrix_1.7-0         jsonlite_1.8.8      \n[49] hms_1.1.3            arrayhelpers_1.1-0   systemfonts_1.1.0   \n[52] ggdist_3.3.2         codetools_0.2-20     distributional_0.4.0\n[55] stringi_1.8.4        gtable_0.3.5         QuickJSR_1.1.3      \n[58] munsell_0.5.1        pillar_1.9.0         htmltools_0.5.8.1   \n[61] Brobdingnag_1.2-9    R6_2.5.1             textshaping_0.4.0   \n[64] evaluate_0.23        lattice_0.22-6       backports_1.5.0     \n[67] memoise_2.0.1        rstantools_2.4.0     coda_0.19-4.1       \n[70] gridExtra_2.3        nlme_3.1-164         checkmate_2.3.1     \n[73] xfun_0.44            pkgconfig_2.0.3"
  },
  {
    "objectID": "posts/quasiquotation/index.html#a-custom-function-with-no-dplyr-verbs",
    "href": "posts/quasiquotation/index.html#a-custom-function-with-no-dplyr-verbs",
    "title": "Write clearer custom functions in R using quasiquotation",
    "section": "A custom function with no dplyr verbs",
    "text": "A custom function with no dplyr verbs\nNow I’ll write a very simple function, double(), that uses no dplyr verbs.\n\ndouble &lt;- function(x){\n  x * 2\n}\n\nIn the function above, x is referred to as a formal argument. Formal arguments like x map to calling argument(s) supplied by the user such as 3 & ds_mt$cyl in the chunk below. This distinction will be important to avoid confusion later on.\n\ndouble(3) # same as double(x = 3)\n\n[1] 6\n\ndouble(x = ds_mt$cyl) # same as double(ds_mt$cyl)\n\n[1] 12 12  8 12 16\n\n\nOur function double() can be used within a dplyr function like mutate() …\n\nds_mt %&gt;% mutate(cyl_2 = double(cyl))\n\n                  cyl cyl_2\nMazda RX4           6    12\nMazda RX4 Wag       6    12\nDatsun 710          4     8\nHornet 4 Drive      6    12\nHornet Sportabout   8    16\n\n\n… but if we write a new version of double() called double_dplyr that has dplyr verbs inside, then mapping to our arguments gets more complicated."
  },
  {
    "objectID": "posts/quasiquotation/index.html#a-custom-function-utilizing-dplyr-verbs",
    "href": "posts/quasiquotation/index.html#a-custom-function-utilizing-dplyr-verbs",
    "title": "Write clearer custom functions in R using quasiquotation",
    "section": "A custom function utilizing dplyr verbs",
    "text": "A custom function utilizing dplyr verbs\nNotice how the function below, double_dplyr(), fails to find cyl within our data ds_mt:\n\ndouble_dplyr &lt;- function(data, x){\n  data %&gt;% \n    mutate(new_var = x * 2)\n}\n\nds_mt %&gt;% double_dplyr(x = cyl)\n\nError in `mutate()`:\nℹ In argument: `new_var = x * 2`.\nCaused by error:\n! object 'cyl' not found\n\n\nIn order for our new function, double_dplyr(), to successfully map its arguments, we need to utilize quasiquotation inside the guts of the function.\nNote: I am not going to get into the weeds of quasiquotation, tidyeval, and nonstandard evaluation in this post. The internet has covered these topics in great detail already. This post is a demonstration of everyday use cases for using quasiquotation to write clear and reliable functions."
  },
  {
    "objectID": "posts/quasiquotation/index.html#bonus-material",
    "href": "posts/quasiquotation/index.html#bonus-material",
    "title": "Write clearer custom functions in R using quasiquotation",
    "section": "Bonus Material:",
    "text": "Bonus Material:\nAnd just because it took me so long to figure out, I’ll include an example that utilizes a formal argument (e.g., groups) that can handle multiple calling arguments, and ..., which allows you to pass optional arguments like “na.rm = TRUE” to nested calls within your function.\n\nmean_by_group &lt;- function(data, x, groups, ...){\n  x &lt;- enquo(x)\n  grp_mean &lt;- paste0(quo_name(x), \"_mean\")\n  \n  groups &lt;- enquos(groups)\n  \n  data %&gt;% \n    group_by_at(vars(!!!groups)) %&gt;% \n    summarise(\n      !!grp_mean := mean(!!x, ...)\n    )\n}\n\n# Example using groups with multiple arguments\nmtcars %&gt;% mean_by_group(x = mpg, groups = c(am, cyl), na.rm = TRUE)\n\n# A tibble: 6 × 3\n# Groups:   am [2]\n     am   cyl mpg_mean\n  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1     0     4     22.9\n2     0     6     19.1\n3     0     8     15.0\n4     1     4     28.1\n5     1     6     20.6\n6     1     8     15.4\n\n# Example using groups with a single argument\nmtcars %&gt;% mean_by_group(x = mpg, groups = cyl, na.rm = TRUE)\n\n# A tibble: 3 × 2\n    cyl mpg_mean\n  &lt;dbl&gt;    &lt;dbl&gt;\n1     4     26.7\n2     6     19.7\n3     8     15.1"
  },
  {
    "objectID": "posts/binomial-deep-dive/index.html",
    "href": "posts/binomial-deep-dive/index.html",
    "title": "Binomial Deep Dive",
    "section": "",
    "text": "pacman::p_load(glue, scales, tidyverse)\n\nformat_range &lt;- function(x){\n    paste0(range(x)[[1]], ' to ', range(x)[[2]])\n}"
  },
  {
    "objectID": "posts/binomial-deep-dive/index.html#summary",
    "href": "posts/binomial-deep-dive/index.html#summary",
    "title": "Binomial Deep Dive",
    "section": "Summary:",
    "text": "Summary:\nSo we’ve just simulated some data using rbinom() and explored its properties."
  },
  {
    "objectID": "posts/simulating-intelligence/index.html",
    "href": "posts/simulating-intelligence/index.html",
    "title": "Simulating Intelligence",
    "section": "",
    "text": "pacman::p_load(glue, scales, tidyverse)\n\n\nA million dollar beer\nMy career took a turn for the better about 5 years ago when I asked a friend who was much stronger in statistics how he got so good. “Data simulation!” he pronounced over his beer. “I simulate data with known relationships and when I can reliably uncover the relationship that I put there … then I trust the new modeling technique. I hardly use any math at the early stages of learning.” I am happy to confirm that this method works really, really well. However, data simulation is rarely taught in a standard statistics course, and so I will offer a primer in this blog.\n\n\nThe likelihood functions are your friends\nYou probably saw many a stack overflow post or textbook example start off with rnorm() or rbinom() and then jump right into a higher level data issue without ever properly introducing you to these wunderkinds. Allow me to offer a proper introduction.\nImagine you have the following scenario\n\nCUSTOMERS &lt;- 1e5\nBASE_RATE &lt;- 0.5\n\nYou have 100,000 customers and historically 50% of them do something that you care about. Maybe it’s a click rate or a graduation rate or show up on time for an appointment. We all care about different things. Pick one that matters to you.\nI work in tech so I care about conversion rates, and I’ll anchor to that. So imagine that historically I see 100,000 new potential customers show up in December and 50% of them convert. But we also know that such point estimates have variance in the real world. We can represent this variance with likelihood functions.\nQuick vocab: each customer is a trial, each conversion is an event, and the binomial function outputs a tally of events within a sample or set of trials. The term “sample” can refer to an individual trialer or to a group of trialers. In this post, “sample” only refers to the group.\n\nset.seed(42) # ensures reproducibility of simulations\n\nevents &lt;- \n    rbinom(\n        n = 1, # samples (where 1 sample = 1 set of trials)\n        size = 10, # trials per sample\n        prob = .5 # Pr(event) in each trial\n    )\n\nevents\n\n[1] 7\n\n\nIn the above example rbinom() simulated 7 events from 1 set of 10 trials. This could represent a sample of “7 heads on 10 flips” or “7 conversions among 10 website visitors.” But this is just one sample.\nWe can also simulate multiple samples:\n\nset.seed(42) # ensures reproducibility of simulations\n\nevents_20 &lt;- \n    rbinom(\n        n = 20, # samples (where 1 sample = 1 set of trials)\n        size = 10, # trials per sample\n        prob = .5 # Pr(event) in each trial\n    )\n\nevents_20\n\n [1] 7 7 4 7 6 5 6 3 6 6 5 6 7 4 5 7 8 3 5 5\n\n\nAs the number of samples grows it becomes more practical to represent the outcomes visually.\n\nset.seed(42) # ensures reproducibility of simulations\n\ndf_1e5 &lt;- \n    tibble(\n        events = \n            rbinom(\n                n = 1e5, # samples (where 1 sample = 1 set of trials)\n                size = 10, # trials per sample\n                prob = .5 # Pr(event) in each trial\n            )\n        )\n\ndf_1e5 |&gt; \n    ggplot(aes(x = events)) + \n    geom_histogram(bins=200) + \n    scale_x_continuous(breaks = 1:10) + \n    labs(\n        title = glue(\"{comma(1e5)} simulated samples from Binomial(10, 0.5)\"), \n        x = \"Simulated Event Tallies\") \n\n\n\n\n\n\n\n\nWhat does this mean? Well, one eye-opening insight for me was that this simulated distribution represents the range and frequency of possible samples that could manifest in 10 trials with a 50% probability of success.\nThere are other ways to represent this information, for instance we can convert these event tallies back into rates, which is often more intuitive.\n\ndf_1e5 |&gt; \n    mutate(rate = events / 10) |&gt; \n    ggplot(aes(x = rate)) + \n    geom_histogram(bins=200) + \n    scale_x_continuous(\n        breaks = pretty_breaks(10),\n        labels = percent) + \n    labs(title = str_wrap(glue(\"Think of this plot as representing the range of observable samples from Binomial(k=10, p=0.5)\")), x = glue(\"Observed Success Rates\\nin 100k Simulated Samples from Binomial(k=10, p=0.5)\"))\n\n\n\n\n\n\n\n\n\nset.seed(42) # ensures reproducibility of simulations\n\nK &lt;- 30\n\ndf_sim_k30_n1e5 &lt;- \n    tibble(\n        events = \n            rbinom(\n                n = 1e5, # samples (where 1 sample = 1 set of trials)\n                size = K, # trials per sample\n                prob = .5 # Pr(event) in each trial\n            ),\n        rate = events / K\n        )\n\ndf_sim_k30_n1e5 |&gt; \n    ggplot(aes(x = events)) + \n    geom_histogram(bins=K+10) + \n    scale_x_continuous(breaks = 1:K) + \n    labs(\n        title = glue(\"{comma(1e5)} simulated samples from Binomial({K}, 0.5)\"), \n        x = \"Simulated Event Tallies\") \n\n\n\n\n\n\n\ndf_sim_k30_n1e5 |&gt; \n    ggplot(aes(x = rate)) + \n    geom_histogram(binwidth = .01) + \n    scale_x_continuous(\n        breaks = pretty_breaks(10),\n        label = percent) + \n    labs(\n        title = glue(\"{comma(1e5)} simulated samples from Binomial({K}, 0.5)\"), \n        \"Observed Rates in Simulated Samples\") \n\n\n\n\n\n\n\n\n\nset.seed(42) # ensures reproducibility of simulations\n\nK &lt;- 1e3\n\ndf_sim_k1e3_n1e5 &lt;- \n    tibble(\n        events = \n            rbinom(\n                n = 1e5, # samples (where 1 sample = 1 set of trials)\n                size = K, # trials per sample\n                prob = .5 # Pr(event) in each trial\n            ),\n        potential_sample_rates = events / K\n        )\n\ndf_sim_k1e3_n1e5 |&gt; \n    ggplot(aes(x = events)) + \n    geom_histogram(bins=K+10) + \n    scale_x_continuous(breaks = 1:K) + \n    labs(\n        title = glue(\"{comma(1e5)} simulated samples from Binomial({K}, 0.5)\"), \n        x = \"Simulated Event Tallies\") \n\n\n\n\n\n\n\ndf_sim_k1e3_n1e5 |&gt; \n    ggplot(aes(x = potential_sample_rates)) + \n    geom_histogram(bins=200) + \n    scale_x_continuous(\n        breaks = pretty_breaks(10),\n        label = percent) + \n    labs(\n        title = glue(\"{comma(1e5)} simulated samples from Binomial({K}, 0.5)\"), \n        x = \"Observed Rates in Simulated Samples\") \n\n\n\n\n\n\n\n\nI think by now you’re starting to see the Central Limit Theorum kick in.\n\ndf_dens_k1e3_n1e5 &lt;- \n    tibble(\n        potential_sample_events = seq(0,K,1),\n        likelihood = dbinom(potential_sample_events, K, BASE_RATE),\n        potential_sample_rates = potential_sample_events / K\n    )\n\ndf_dens_k1e3_n1e5 |&gt; \n    ggplot(aes(x = potential_sample_events, y = likelihood)) + \n    geom_line() + \n    scale_x_continuous(\n        breaks = \n    ) + \n    coord_cartesian(xlim = c(430, 570))\n\n\n\n\n\n\n\n\n\ndf_dens_k1e3_n1e5 |&gt; \n    ggplot(aes(x = potential_sample_rates, y = likelihood)) + \n    geom_line() + \n    scale_x_continuous(\n        breaks = pretty_breaks(10),\n        labels = percent\n    ) + \n    coord_cartesian(xlim = c(.430, .570)) + \n    labs(y = \"likelihood of observing any rate in a sample\")\n\n\n\n\n\n\n\n\nWORKING HERE: I was hoping to overlay these plots\n\n\nDemo pbinom() vs rbinom()\n\npbinom(.5*K, size = K, prob = BASE_RATE) # P[X≤x]\n\n[1] 0.5126125\n\n\nPretty close…\n\nmean(df_sim_k1e3_n1e5$events &lt;= .5*K)\n\n[1] 0.51474\n\n\n\nqbinom(.025, size = K, prob = BASE_RATE) \n\n[1] 469\n\nqbinom(.975, size = K, prob = BASE_RATE) \n\n[1] 531\n\n\nDead match!\n\nquantile(df_sim_k1e3_n1e5$events, c(.025, .975))\n\n 2.5% 97.5% \n  469   531 \n\n\n\n\nOverlay plots\n\ndemo_K &lt;- 100\n\ndf1 &lt;- tibble(\n    x = seq(0,demo_K,1),\n    y = dbinom(x, demo_K, .5)\n)\n\ndf2 &lt;- tibble(\n    x = rbinom(1e5, demo_K, .5)\n) |&gt; \n    count(x) |&gt; \n    mutate(\n        prop = n / sum(n)\n    )\n\n# this join works very well to create a double y-axis\nleft_join(df1, df2, by = 'x') |&gt; \n    ggplot(aes(x = x, y = y)) + \n    geom_col(aes(x=x, y=prop)) + \n    geom_line(color = 'red') + \n    scale_x_continuous(\n        breaks = pretty_breaks(10),\n        labels = ~percent(.x/demo_K)\n        ) + \n    scale_y_continuous(\n        sec.axis = \n            sec_axis(\n                trans = ~.x*demo_K, \n                name = \"Tally of Simulated Samples\"\n            ) \n    ) + \n    labs(\n        x = \"Potential Sample Rates\",\n        y = \"Likelihood of Sample Rate\",\n        title = str_wrap(\"Note the convergence of rbinom()'s histogram and dbinom()'s line\"))\n\nWarning: The `trans` argument of `sec_axis()` is deprecated as of ggplot2 3.5.0.\nℹ Please use the `transform` argument instead.\n\n\nWarning: Removed 58 rows containing missing values or values outside the scale range\n(`geom_col()`).\n\n\n\n\n\n\n\n\n\nNote how the likelihood density values from a discrete probability distribution like the binomnial are highly interpretable. For instance there is literally an 8% probability that \\(Binomial(k=100,p=8)\\) will manifest a sample rate of 50%.\nWhen we dive into the normal distribution, you will see that the likelihood\nimage source"
  },
  {
    "objectID": "setup_and_publishing.html",
    "href": "setup_and_publishing.html",
    "title": "setup",
    "section": "",
    "text": "Publishing\nquarto render then quarto publish --no-prompt although quarto render may have been superflous\n\n\nSetup\nhttps://quarto.org/docs/websites/website-blog.html#:~:text=To%20create%20a%20new%20blog%20project%20within%20RStudio%2C%20use%20the,right%20of%20the%20source%20file.\n\n\nPublishing\nexecute quarto publish in the terminal, and then click Enter to publish to your website.\n\n\n(Un)Freezing\nI often need to delete folders in the _freeze directory in order for updates to appear in previously published posts.\n\n\nyour blog will be visible at\nhttps://joepowers16.quarto.pub/throughthepuppethouse/\nquarto preview works\nquarto render\nquarto publish --no-prompt"
  }
]